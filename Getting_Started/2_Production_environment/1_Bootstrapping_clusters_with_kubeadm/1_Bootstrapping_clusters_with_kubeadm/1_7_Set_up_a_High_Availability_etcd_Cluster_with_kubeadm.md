# 1.7 Set up a High Availability etcd Cluster with kubeadm

## ğŸš€ Set up a High Availability etcd Cluster with kubeadm / HochverfÃ¼gbaren etcd-Cluster mit kubeadm einrichten


**Deutsche Ãœbersetzung:**  
StandardmÃ¤ÃŸig betreibt **kubeadm** auf jedem Control-Plane-Node eine lokale **etcd**-Instanz.  
Alternativ kann der etcd-Cluster **extern** betrieben werden, also auf separaten Hosts.  
Die Unterschiede zwischen diesen beiden AnsÃ¤tzen werden in der Seite  
**[Options for Highly Available Topology](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/)** erlÃ¤utert.

Diese Anleitung zeigt Schritt fÃ¼r Schritt, wie man einen **hochverfÃ¼gbaren externen etcd-Cluster** mit **drei Mitgliedern** erstellt, der anschlieÃŸend von **kubeadm** zur Cluster-Erstellung genutzt werden kann.


---

## ğŸ§© Before you begin / Voraussetzungen


**Deutsche Ãœbersetzung:**  
Du benÃ¶tigst:

- **Drei Hosts**, die Ã¼ber TCP-Ports **2379** und **2380** miteinander kommunizieren kÃ¶nnen  
  (Standardwerte, anpassbar Ã¼ber die kubeadm-Konfigurationsdatei).  
- Jeder Host muss **systemd** und eine **bash-kompatible Shell** installiert haben.  
- Jeder Host benÃ¶tigt eine **Container-Runtime**, den **kubelet** und **kubeadm**.  
- Zugriff auf das Kubernetes-Image-Repository (**registry.k8s.io**) oder manuelles Laden der etcd-Images Ã¼ber:  
  ```bash
  kubeadm config images list
  kubeadm config images pull
````

etcd wird als **static Pod** vom kubelet verwaltet.

* MÃ¶glichkeit zum Kopieren von Dateien zwischen Hosts (z. B. via **ssh** und **scp**).

> **Hinweis:**
> kubeadm enthÃ¤lt alle notwendigen kryptografischen Werkzeuge zur Zertifikatserstellung.
> Es sind keine weiteren Tools nÃ¶tig.

> **IPv6-UnterstÃ¼tzung:**
> kubeadm, kubelet und etcd kÃ¶nnen IPv6-Adressen verwenden.
> Dual-Stack wird von Kubernetes teilweise unterstÃ¼tzt, jedoch **nicht von etcd**.

---

## âš™ï¸ Setting up the cluster / Cluster einrichten

### ğŸ”§ kubelet als Service-Manager fÃ¼r etcd konfigurieren

FÃ¼hre diesen Schritt **auf allen Hosts** aus, auf denen etcd laufen soll.
Da etcd vor Kubernetes erstellt wird, muss die Standard-Unit-Datei des kubelet durch eine eigene Konfiguration mit hÃ¶herer PrioritÃ¤t ersetzt werden.

```bash
cat << EOF > /etc/systemd/system/kubelet.service.d/kubelet.conf
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: false
authorization:
  mode: AlwaysAllow
cgroupDriver: systemd
address: 127.0.0.1
containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock
staticPodPath: /etc/kubernetes/manifests
EOF
```

```bash
cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
ExecStart=/usr/bin/kubelet --config=/etc/systemd/system/kubelet.service.d/kubelet.conf
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet
```

---

### ğŸ§± Konfigurationsdateien fÃ¼r kubeadm erstellen

Erstelle fÃ¼r jeden Host eine eigene **kubeadmcfg.yaml**, die die lokalen etcd-Einstellungen enthÃ¤lt.

```bash
# Beispiel: IPs und Hostnamen anpassen
export HOST0=10.0.0.6
export HOST1=10.0.0.7
export HOST2=10.0.0.8

export NAME0="infra0"
export NAME1="infra1"
export NAME2="infra2"

mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/
HOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=(${NAME0} ${NAME1} ${NAME2})

for i in "${!HOSTS[@]}"; do
HOST=${HOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
---
apiVersion: "kubeadm.k8s.io/v1beta4"
kind: InitConfiguration
nodeRegistration:
    name: ${NAME}
localAPIEndpoint:
    advertiseAddress: ${HOST}
---
apiVersion: "kubeadm.k8s.io/v1beta4"
kind: ClusterConfiguration
etcd:
    local:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
        - name: initial-cluster
          value: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380
        - name: initial-cluster-state
          value: new
        - name: name
          value: ${NAME}
        - name: listen-peer-urls
          value: https://${HOST}:2380
        - name: listen-client-urls
          value: https://${HOST}:2379
        - name: advertise-client-urls
          value: https://${HOST}:2379
        - name: initial-advertise-peer-urls
          value: https://${HOST}:2380
EOF
done
```

---

### ğŸ” Zertifikate generieren

#### 1ï¸âƒ£ Certificate Authority (CA)

Falls bereits eine CA vorhanden ist, kopiere deren Zertifikat und SchlÃ¼ssel nach:

```
/etc/kubernetes/pki/etcd/ca.crt
/etc/kubernetes/pki/etcd/ca.key
```

Falls keine vorhanden ist, erstelle sie auf `$HOST0`:

```bash
kubeadm init phase certs etcd-ca
```

Dadurch entstehen:

```
/etc/kubernetes/pki/etcd/ca.crt
/etc/kubernetes/pki/etcd/ca.key
```

#### 2ï¸âƒ£ Zertifikate fÃ¼r jedes Mitglied

Erstelle nacheinander alle notwendigen Zertifikate fÃ¼r die drei etcd-Mitglieder:

```bash
kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
```

> Wiederhole diesen Schritt analog fÃ¼r `${HOST1}` und `${HOST0}`.
> Entferne die `ca.key`-Dateien aus den temporÃ¤ren Verzeichnissen von HOST1/HOST2, bevor du sie Ã¼bertrÃ¤gst.

---

### ğŸ“¤ Zertifikate und Konfigurationen Ã¼bertragen

Kopiere nun die generierten Dateien auf die Zielsysteme:

```bash
USER=ubuntu
HOST=${HOST1}
scp -r /tmp/${HOST}/* ${USER}@${HOST}:
ssh ${USER}@${HOST}
sudo -Es
chown -R root:root pki
mv pki /etc/kubernetes/
```

ÃœberprÃ¼fe anschlieÃŸend, dass alle erforderlichen Dateien vorhanden sind.
Beispiel fÃ¼r `$HOST0`:

```
/etc/kubernetes/pki/
â”œâ”€â”€ apiserver-etcd-client.crt
â”œâ”€â”€ apiserver-etcd-client.key
â””â”€â”€ etcd/
    â”œâ”€â”€ ca.crt
    â”œâ”€â”€ ca.key
    â”œâ”€â”€ healthcheck-client.crt
    â”œâ”€â”€ healthcheck-client.key
    â”œâ”€â”€ peer.crt
    â”œâ”€â”€ peer.key
    â”œâ”€â”€ server.crt
    â””â”€â”€ server.key
```

---

### âš™ï¸ etcd Static Pod-Manifeste erstellen

Nachdem Zertifikate und Konfigurationen bereitstehen, erstelle auf **jedem Host** das etcd-Manifest:

```bash
root@HOST0 $ kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml
root@HOST1 $ kubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml
root@HOST2 $ kubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml
```

---

### âœ… Cluster-Gesundheit prÃ¼fen (optional)

Mit **etcdctl**:

```bash
ETCDCTL_API=3 etcdctl \
--cert /etc/kubernetes/pki/etcd/peer.crt \
--key /etc/kubernetes/pki/etcd/peer.key \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--endpoints https://${HOST0}:2379 endpoint health
```

Beispielausgabe:

```
https://10.0.0.6:2379 is healthy: successfully committed proposal
https://10.0.0.7:2379 is healthy: successfully committed proposal
https://10.0.0.8:2379 is healthy: successfully committed proposal
```

---

## ğŸ§­ What's next / NÃ¤chste Schritte

**Deutsche Ãœbersetzung:**
Sobald dein etcd-Cluster mit **drei funktionsfÃ¤higen Mitgliedern** lÃ¤uft, kannst du fortfahren, eine **hochverfÃ¼gbare Control Plane** mit **kubeadm** einzurichten â€” unter Verwendung der **externen etcd-Topologie**.

ğŸ‘‰ Siehe dazu: **[Creating Highly Available Clusters with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)**.


