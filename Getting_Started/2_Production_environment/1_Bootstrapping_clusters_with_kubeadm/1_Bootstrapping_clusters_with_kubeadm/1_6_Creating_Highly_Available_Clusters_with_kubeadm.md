# 1.6 Creating Highly Available Clusters with kubeadm


## üöÄ Creating Highly Available Clusters with kubeadm / Hochverf√ºgbare Cluster mit kubeadm erstellen


**Deutsche √úbersetzung:**  
Diese Seite beschreibt zwei verschiedene Ans√§tze zum Einrichten eines **hochverf√ºgbaren (HA) Kubernetes-Clusters** mit **kubeadm**:

1. **Gestapelte Control-Plane-Nodes**  
   ‚Äì Weniger Infrastruktur erforderlich; `etcd` und Control-Plane-Komponenten laufen auf denselben Nodes.  
2. **Externer etcd-Cluster**  
   ‚Äì Mehr Infrastruktur erforderlich; `etcd` und Control Plane sind getrennt.

W√§hle die Topologie, die den Anforderungen deiner Umgebung und Anwendungen am besten entspricht.  
Siehe auch: **[Options for Highly Available Topology](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/)**.

> **Achtung:**  
> Diese Anleitung gilt nicht f√ºr Cloud-Provider-Setups. In Cloud-Umgebungen funktionieren weder gestapelte noch externe HA-Topologien direkt mit  
> - Services vom Typ **LoadBalancer**  
> - noch mit **dynamischen PersistentVolumes**.  


---

## üß© Before you begin / Vorbereitungen


**Deutsche √úbersetzung:**  
Abh√§ngig von der gew√§hlten Topologie (**stacked etcd** oder **external etcd**) ben√∂tigst du:

- Mindestens **drei Control-Plane-Nodes** (besser ungerade Zahl f√ºr Leader-Auswahl)  
- Mindestens **drei Worker-Nodes**  
- Alle Maschinen m√ºssen kubeadm‚Äôs **Mindestanforderungen** erf√ºllen  
- **Container-Runtime** installiert und funktionsf√§hig  
- **Volle Netzwerkkonnektivit√§t** zwischen allen Maschinen  
- **sudo-Zugriff (Superuser-Rechte)**  
- **SSH-Zugriff** von einer Steuerungsmaschine auf alle Nodes  
- **kubeadm** und **kubelet** bereits installiert  

> Siehe auch: [Stacked etcd topology](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology)

### Container-Images
Jeder Host muss Images aus **registry.k8s.io** ziehen k√∂nnen.  
Falls das nicht m√∂glich ist, m√ºssen die Images manuell auf allen Hosts vorhanden sein.

### Command-Line Interface
Installiere `kubectl` auf deinem PC sowie optional auf jedem Control-Plane-Node, um Fehlerdiagnosen zu erleichtern.


---

## ‚öôÔ∏è First steps for both methods / Erste Schritte f√ºr beide Methoden


### üß≠ Load Balancer f√ºr kube-apiserver erstellen

Erstelle einen Load Balancer mit einer DNS-Adresse (nicht IP-basiert in Cloud-Umgebungen).  
Dieser verteilt den Traffic auf alle gesunden Control-Plane-Nodes.  
Health-Check: TCP-Port **6443** (Standard f√ºr kube-apiserver).

√úberpr√ºfe die Verbindung:
```bash
nc -zv -w 2 <LOAD_BALANCER_IP> <PORT>
````

Ein *connection refused* ist erwartbar, bevor der API-Server l√§uft; *timeout* bedeutet falsche Konfiguration.

> Die Load-Balancer-Adresse muss mit der in **ControlPlaneEndpoint** verwendeten √ºbereinstimmen.

---

## üß± Stacked control plane and etcd nodes / Gestapelte Control-Plane- und etcd-Nodes

### üèÅ Erster Control-Plane-Node

Initialisiere den Cluster:

```bash
sudo kubeadm init --control-plane-endpoint "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" --upload-certs
```

Optionen:

* `--kubernetes-version`: gew√ºnschte Kubernetes-Version
* `--upload-certs`: l√§dt Zertifikate verschl√ºsselt ins Cluster hoch
* `--pod-network-cidr`: legt das Pod-Netzwerk fest (bei manchen CNI-Plugins erforderlich)

> **Hinweis:**
>
> * `--config` und `--certificate-key` d√ºrfen nicht kombiniert werden.
> * Bei Nutzung einer Konfigurationsdatei muss `certificateKey` unter `InitConfiguration.controlPlane` gesetzt werden.

Nach erfolgreichem Init wird eine `join`-Anweisung angezeigt, z. B.:

```bash
kubeadm join 192.168.0.200:6443 --token <TOKEN> --discovery-token-ca-cert-hash <HASH> --control-plane --certificate-key <KEY>
```

* `--control-plane` ‚Üí erstellt eine neue Control-Plane-Instanz
* `--certificate-key` ‚Üí l√§dt die verschl√ºsselten Zertifikate herunter

> **Wichtig:**
>
> * Der Schl√ºssel ist **sensitiv** und nur **2 Stunden g√ºltig**.
> * Danach kann er mit
>
>   ```bash
>   sudo kubeadm init phase upload-certs --upload-certs
>   ```
>
>   erneut erzeugt werden.

### üß© Netzwerk einrichten

Installiere ein CNI-Plugin deiner Wahl.
Achte darauf, dass das Plugin mit dem Pod-CIDR √ºbereinstimmt.

√úberwache den Start der Control-Plane-Komponenten:

```bash
kubectl get pod -n kube-system -w
```

### ‚ûï Weitere Control-Plane-Nodes hinzuf√ºgen

F√ºhre auf jedem weiteren Node den zuvor ausgegebenen Join-Befehl aus:

```bash
sudo kubeadm join <LOAD_BALANCER_DNS>:6443 --token <TOKEN> --discovery-token-ca-cert-hash <HASH> --control-plane --certificate-key <KEY>
```

> **Tipp:**
> Nach dem Hinzuf√ºgen weiterer Nodes sollte man CoreDNS neu verteilen, um h√∂here Verf√ºgbarkeit zu erreichen:
>
> ```bash
> kubectl -n kube-system rollout restart deployment coredns
> ```

---

## üåê External etcd nodes / Externe etcd-Nodes

### üì¶ etcd-Cluster einrichten

1. Richte den etcd-Cluster gem√§√ü der offiziellen [etcd-Dokumentation](https://etcd.io/docs/latest/op-guide/clustering/) ein.
2. Kopiere die folgenden Zertifikate von einem etcd-Node auf den ersten Control-Plane-Node:

   ```bash
   export CONTROL_PLANE="ubuntu@10.0.0.7"
   scp /etc/kubernetes/pki/etcd/ca.crt "${CONTROL_PLANE}":
   scp /etc/kubernetes/pki/apiserver-etcd-client.crt "${CONTROL_PLANE}":
   scp /etc/kubernetes/pki/apiserver-etcd-client.key "${CONTROL_PLANE}":
   ```

### ‚öôÔ∏è Control Plane konfigurieren

Erstelle die Datei `kubeadm-config.yaml`:

```yaml
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT"
etcd:
  external:
    endpoints:
      - https://ETCD_0_IP:2379
      - https://ETCD_1_IP:2379
      - https://ETCD_2_IP:2379
    caFile: /etc/kubernetes/pki/etcd/ca.crt
    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
```

Initialisiere die Control Plane:

```bash
sudo kubeadm init --config kubeadm-config.yaml --upload-certs
```

Danach:

* Join-Kommandos speichern
* CNI-Plugin installieren
* Weitere Control-Plane-Nodes wie bei stacked etcd hinzuf√ºgen

---

## ‚öôÔ∏è Manual certificate distribution / Manuelle Zertifikatsverteilung

**Deutsche √úbersetzung:**
Wenn `--upload-certs` **nicht verwendet** wird, m√ºssen Zertifikate **manuell kopiert** werden.

### üîë Vorgehen:

1. SSH-Agent aktivieren und Schl√ºssel laden:

   ```bash
   eval $(ssh-agent)
   ssh-add ~/.ssh/path_to_private_key
   ```
2. Verbindung testen mit:

   ```bash
   ssh -A <NODE_IP>
   ```
3. Zertifikate vom ersten Control-Plane-Node auf alle weiteren kopieren:

   ```bash
   USER=ubuntu
   CONTROL_PLANE_IPS="10.0.0.7 10.0.0.8"
   for host in ${CONTROL_PLANE_IPS}; do
       scp /etc/kubernetes/pki/ca.crt "${USER}"@$host:
       scp /etc/kubernetes/pki/ca.key "${USER}"@$host:
       scp /etc/kubernetes/pki/sa.key "${USER}"@$host:
       scp /etc/kubernetes/pki/sa.pub "${USER}"@$host:
       scp /etc/kubernetes/pki/front-proxy-ca.crt "${USER}"@$host:
       scp /etc/kubernetes/pki/front-proxy-ca.key "${USER}"@$host:
       scp /etc/kubernetes/pki/etcd/ca.crt "${USER}"@$host:etcd-ca.crt
       scp /etc/kubernetes/pki/etcd/ca.key "${USER}"@$host:etcd-ca.key
   done
   ```

> ‚ö†Ô∏è **Achtung:**
> Kopiere **nur die genannten Zertifikate**, nicht alle!
> kubeadm generiert die restlichen automatisch mit den richtigen SANs.

Auf jedem beitretenden Control-Plane-Node:

```bash
USER=ubuntu
mkdir -p /etc/kubernetes/pki/etcd
mv /home/${USER}/ca.crt /etc/kubernetes/pki/
mv /home/${USER}/ca.key /etc/kubernetes/pki/
mv /home/${USER}/sa.key /etc/kubernetes/pki/
mv /home/${USER}/sa.pub /etc/kubernetes/pki/
mv /home/${USER}/front-proxy-ca.crt /etc/kubernetes/pki/
mv /home/${USER}/front-proxy-ca.key /etc/kubernetes/pki/
mv /home/${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /home/${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
```

---

## ‚úÖ Common tasks after bootstrapping / H√§ufige Aufgaben nach dem Setup

**Deutsche √úbersetzung:**

* Worker-Nodes beitreten lassen:

  ```bash
  sudo kubeadm join 192.168.0.200:6443 --token <TOKEN> --discovery-token-ca-cert-hash <HASH>
  ```
* Cluster-Funktionalit√§t pr√ºfen (`kubectl get nodes`)
* Netzwerk-Plugin validieren
* Zertifikate regelm√§√üig pr√ºfen und erneuern

