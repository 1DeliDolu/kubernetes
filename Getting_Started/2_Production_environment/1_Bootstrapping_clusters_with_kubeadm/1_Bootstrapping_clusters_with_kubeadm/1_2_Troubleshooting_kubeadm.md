# 1.2 Troubleshooting kubeadm

## ğŸš€ Troubleshooting kubeadm / Fehlerbehebung mit kubeadm


**Deutsche Ãœbersetzung:**  
Wie bei jedem Programm kann es auch bei der Installation oder Nutzung von **kubeadm** zu Fehlern kommen.  
Diese Seite listet **hÃ¤ufige Fehlerszenarien** auf und beschreibt mÃ¶gliche LÃ¶sungen.

Falls dein Problem hier nicht aufgefÃ¼hrt ist, gehe wie folgt vor:

1. PrÃ¼fe auf [GitHub](https://github.com/kubernetes/kubeadm), ob ein bestehendes Issue vorhanden ist.  
2. Falls nicht, erÃ¶ffne ein neues Issue und folge dem Template.  
3. Wenn du dir unsicher bist, stelle Fragen im **Slack-Channel `#kubeadm`** oder auf **StackOverflow** mit den Tags `#kubernetes` und `#kubeadm`.  


---

## ğŸ”‘ HÃ¤ufige Probleme und LÃ¶sungen


### âŒ Node-Beitritt schlÃ¤gt fehl (v1.18 Node zu v1.17 Cluster)
- Ab v1.18 benÃ¶tigt kubeadm zusÃ¤tzliche **RBAC-Berechtigungen** fÃ¼r den bootstrap-token.  
- LÃ¶sungsmÃ¶glichkeiten:  
  - Auf einem Control-Plane-Node mit v1.18:  
    ```bash
    kubeadm init phase bootstrap-token
    ```  
  - Oder RBAC manuell hinzufÃ¼gen:  
    ```yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: kubeadm:get-nodes
    rules:
      - apiGroups: [""]
        resources: ["nodes"]
        verbs: ["get"]
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: kubeadm:get-nodes
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: kubeadm:get-nodes
    subjects:
      - apiGroup: rbac.authorization.k8s.io
        kind: Group
        name: system:bootstrappers:kubeadm:default-node-token
    ```


### âš ï¸ ebtables oder ethtool fehlen
- Warnungen bei `kubeadm init`:  
````

[preflight] WARNING: ebtables not found in system path
[preflight] WARNING: ethtool not found in system path

````
- LÃ¶sung:  
- Ubuntu/Debian: `apt install ebtables ethtool`  
- CentOS/Fedora: `yum install ebtables ethtool`  


### â³ kubeadm hÃ¤ngt bei â€waiting for the control planeâ€œ
MÃ¶gliche Ursachen:
- Netzwerkprobleme â†’ volle KonnektivitÃ¤t sicherstellen.  
- Unterschiedliche **cgroup drivers** bei Runtime und kubelet â†’ siehe [cgroup driver konfigurieren](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/).  
- Control Plane-Container crashen â†’ prÃ¼fen mit:  
```bash
docker ps
docker logs <container>
````

oder bei CRI-Runtime: `crictl` nutzen.

### ğŸ”„ kubeadm reset hÃ¤ngt beim Entfernen von Containern

* Ursache: Container-Runtime blockiert.
* LÃ¶sung: Runtime neu starten und `kubeadm reset` erneut ausfÃ¼hren.
* Debugging: `crictl` verwenden.

### ğŸ³ Pods in CrashLoopBackOff oder Error

* Direkt nach `kubeadm init` darf dies nicht auftreten (auÃŸer `coredns` im Pending-Status, bis das Netzwerk-Plugin installiert ist).
* Wenn nach Deployment des CNI-Plugins Pods fehlschlagen â†’ Plugin-Konfiguration prÃ¼fen oder aktualisieren.

### ğŸ“¡ coredns bleibt im Pending-Status

* Erwartetes Verhalten: kubeadm ist **netzwerkagnostisch**.
* LÃ¶sung: **Pod-Netzwerk-Plugin installieren**, dann startet CoreDNS.

### ğŸŒ HostPort Services funktionieren nicht

* AbhÃ¤ngig vom CNI-Provider.
* UnterstÃ¼tzt: **Calico, Canal, Flannel**.
* Alternativ: `NodePort` oder `hostNetwork=true` verwenden.

### ğŸ”„ Pods nicht Ã¼ber ihre Service-IP erreichbar

* Ursache: fehlender **Hairpin-Mode** im Netzwerkplugin.
* LÃ¶sung: Anbieter kontaktieren oder Workaround Ã¼ber `/etc/hosts`.
* Bei VirtualBox: sicherstellen, dass `hostname -i` eine routbare IP zurÃ¼ckliefert.

### ğŸ” TLS-Zertifikatfehler

Fehlerbeispiel:

```
Unable to connect to the server: x509: certificate signed by unknown authority
```

LÃ¶sungen:

* PrÃ¼fe `$HOME/.kube/config` und erneuere ggf. Zertifikate.
* Standard-Config setzen:

  ```bash
  export KUBECONFIG=/etc/kubernetes/admin.conf
  ```
* Admin kubeconfig neu erstellen:

  ```bash
  mv $HOME/.kube $HOME/.kube.bak
  mkdir $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  ```

### ğŸ”‘ Kubelet-Zertifikatrotation schlÃ¤gt fehl

* Alte Zertifikate lÃ¶schen (`/etc/kubernetes/kubelet.conf`, `/var/lib/kubelet/pki/`).
* Neues kubelet-Config vom Control Plane erzeugen:

  ```bash
  kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf
  ```
* Auf fehlerhaften Node kopieren und kubelet neustarten.

### ğŸ”„ etcd-Pods starten stÃ¤ndig neu

* Betrifft **CentOS 7 + Docker 1.13.1.84**.
* LÃ¶sung:

  * Downgrade auf `1.13.1-75`
  * oder Upgrade auf **Docker 18.06+**.

### âš™ï¸ kubeadm init extra args Problem (Komma-getrennte Werte)

* Flags wie `--apiserver-extra-args` unterstÃ¼tzen keine Komma-getrennten Listen.
* Workaround: **kubeadm-Konfigurationsdatei** verwenden.

### ğŸ“¦ kube-proxy startet zu frÃ¼h (Cloud-Controller nicht ready)

* Fehler: kube-proxy erhÃ¤lt `127.0.0.1` als NodeIP.
* LÃ¶sung: Patch des DaemonSets mit Tolerations, sodass es auf Control-Plane lÃ¤uft, bis Cloud-Controller fertig ist.

### ğŸ”’ /usr ist read-only (z. B. Fedora CoreOS)

* Problem bei **FlexVolume**.
* LÃ¶sung: In kubeadm-Konfigurationsdatei einen alternativen Volume-Pfad setzen.
* Alternativ `/usr` beschreibbar mounten (nicht empfohlen).

### ğŸ“ˆ metrics-server TLS-Problem

* Ursache: kubeadm setzt self-signed Zertifikate.
* LÃ¶sung: kubelets mit signierten Zertifikaten konfigurieren â†’ siehe [Signed kubelet serving certificates](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubelet-integration/#enabling-signed-kubelet-serving-certificates).

### ğŸ”„ Upgrade-Probleme mit etcd-Hash (v1.28.x â†’ v1.28.3+)

* Fehler: etcd Pod-Hash Ã¤ndert sich nicht.
* Workarounds:

  * `--etcd-upgrade=false` beim Upgrade (nicht empfohlen).
  * Oder etcd-Manifest vorab patchen und fehlerhafte Defaults entfernen.

---

## ğŸ’¡ Zusammenfassung

**Deutsche Ãœbersetzung:**
Die hÃ¤ufigsten kubeadm-Probleme betreffen:

* VersionsinkompatibilitÃ¤ten
* Fehlende AbhÃ¤ngigkeiten (z. B. ebtables, ethtool)
* Netzwerkkonfiguration (CNI, Hairpin-Mode, HostPort)
* Zertifikatsprobleme (TLS, Kubelet-Rotation)
* Laufzeitprobleme mit etcd oder Container-Runtime

ğŸ‘‰ FÃ¼r detaillierte Debugging-Hilfe: siehe [Debugging Kubernetes nodes with crictl](https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/).

