# Running in multiple zones


## ğŸš€ Background / Hintergrund


**Deutsche Ãœbersetzung:**  
Kubernetes ist so konzipiert, dass ein einzelnes Kubernetes-Cluster Ã¼ber mehrere **Failure Zones** hinweg betrieben werden kann â€“ typischerweise innerhalb einer logischen Gruppierung, die als **Region** bezeichnet wird. GroÃŸe Cloud-Anbieter definieren eine Region als eine Gruppe von Failure Zones (auch **Availability Zones** genannt), die denselben Funktionsumfang bieten: Innerhalb einer Region stellen alle Zonen die gleichen **APIs** und **Dienste** bereit.

Typische Cloud-Architekturen zielen darauf ab, die Wahrscheinlichkeit zu minimieren, dass ein Ausfall in einer Zone auch Dienste in anderen Zonen beeintrÃ¤chtigt.


## ğŸ§  Control plane behavior / Verhalten der Control Plane


Alle Control-Plane-Komponenten unterstÃ¼tzen den Betrieb als **Pool austauschbarer Ressourcen**, wobei jede Komponente **repliziert** wird.

Beim Bereitstellen der **Cluster-Control-Plane** solltest du die Replikate der Control-Plane-Komponenten **Ã¼ber mehrere Failure Zones** verteilen.  
Wenn **VerfÃ¼gbarkeit** eine hohe PrioritÃ¤t hat, wÃ¤hle **mindestens drei Failure Zones** und repliziere jede einzelne Control-Plane-Komponente  
(`API Server`, `Scheduler`, `etcd`, `Controller Manager`) Ã¼ber **mindestens drei Zonen** hinweg.  
Falls du einen **Cloud Controller Manager** verwendest, solltest du diesen ebenfalls Ã¼ber alle ausgewÃ¤hlten Failure Zones hinweg replizieren.

> **Hinweis:**  
> Kubernetes bietet keine eingebaute **Zonen-Resilienz** fÃ¼r die API-Server-Endpunkte.  
> Du kannst jedoch verschiedene Techniken einsetzen, um die **VerfÃ¼gbarkeit** des Cluster-API-Servers zu verbessern â€“ etwa **DNS Round-Robin**, **SRV-Records** oder **Load-Balancer-LÃ¶sungen** mit **Health Checks**.


## ğŸ§© Node behavior / Verhalten der Nodes


Kubernetes verteilt automatisch die **Pods von Workloads** (z. B. `Deployment` oder `StatefulSet`) auf verschiedene Nodes im Cluster. Diese Verteilung reduziert die Auswirkungen von AusfÃ¤llen.

Wenn Nodes gestartet werden, fÃ¼gt der **kubelet** auf jedem Node automatisch **Labels** zum zugehÃ¶rigen `Node`-Objekt in der API hinzu. Diese Labels kÃ¶nnen **Zoneninformationen** enthalten.

Wenn dein Cluster Ã¼ber mehrere Zonen oder Regionen hinweg lÃ¤uft, kannst du diese Node-Labels zusammen mit **Pod Topology Spread Constraints** verwenden, um zu steuern, wie Pods Ã¼ber Regionen, Zonen oder bestimmte Nodes verteilt werden.  
Diese Hinweise helfen dem **Scheduler**, Pods so zu platzieren, dass die **erwartete VerfÃ¼gbarkeit** erhÃ¶ht und das Risiko von **korrelierten AusfÃ¤llen** verringert wird.

Beispiel:  
Du kannst eine **Constraint** definieren, die sicherstellt, dass die drei Replikate eines `StatefulSet` jeweils in **unterschiedlichen Zonen** laufen, sofern dies mÃ¶glich ist â€“ ohne explizit anzugeben, welche Zonen verwendet werden sollen.


## ğŸŒ Distributing nodes across zones / Verteilung von Nodes Ã¼ber Zonen


Der Kubernetes-Kern erstellt **keine Nodes automatisch** â€“ du musst sie selbst erstellen oder ein Tool wie die **Cluster API** verwenden, um Nodes fÃ¼r dich zu verwalten.

Mit Tools wie der **Cluster API** kannst du:
- **MaschinensÃ¤tze** definieren, die als **Worker Nodes** Ã¼ber mehrere Failure Domains verteilt werden,
- **Automatische Heilungsregeln** konfigurieren, die dein Cluster wiederherstellen, wenn ganze Zonen ausfallen.


## ğŸ¯ Manual zone assignment for Pods / Manuelle Zonenzuweisung fÃ¼r Pods


Du kannst **Node-Selector-Constraints** auf Pods anwenden, die du erstellst, sowie auf Pod-Vorlagen in Workload-Ressourcen wie `Deployment`, `StatefulSet` oder `Job`.


## ğŸ’¾ Storage access for zones / Speicherzugriff pro Zone


Wenn **Persistente Volumes (PVs)** erstellt werden, fÃ¼gt Kubernetes automatisch **Zonenlabels** zu diesen Volumes hinzu, wenn sie an eine bestimmte Zone gebunden sind.  
Der Scheduler stellt mithilfe des **NoVolumeZoneConflict**-PrÃ¤dikats sicher, dass Pods, die ein bestimmtes PV anfordern, nur in derselben Zone platziert werden, in der sich das Volume befindet.

Beachte, dass das Verfahren zum HinzufÃ¼gen von Zonenlabels vom **Cloud-Anbieter** und dem verwendeten **Storage Provisioner** abhÃ¤ngen kann.  
Konsultiere immer die spezifische Dokumentation deiner Umgebung, um eine korrekte Konfiguration sicherzustellen.

Du kannst in einer `StorageClass` fÃ¼r deine `PersistentVolumeClaims` die **Failure Domains (Zonen)** festlegen, in denen der Speicher verwendet werden darf.  
Siehe dazu: [Allowed topologies](https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies).


## ğŸŒ Networking / Netzwerk


Kubernetes selbst bietet **keine zonenbewusste Netzwerkkonfiguration**.  
Du kannst jedoch ein **Netzwerk-Plugin** verwenden, um das Cluster-Netzwerk zu konfigurieren â€“ und dieses Plugin kann zonenspezifische Eigenschaften haben.

Beispiel:  
Wenn dein Cloud-Anbieter `Services` mit `type=LoadBalancer` unterstÃ¼tzt, kann der Load Balancer den Datenverkehr nur an Pods in derselben Zone weiterleiten, in der der jeweilige Load-Balancer-Knoten lÃ¤uft.  
ÃœberprÃ¼fe hierzu die **Dokumentation deines Cloud-Anbieters**.

FÃ¼r **eigene oder On-Premises-Deployments** gelten Ã¤hnliche Ãœberlegungen.  
Das Verhalten von **Service** und **Ingress**, insbesondere im Hinblick auf den Umgang mit unterschiedlichen Zonen, hÃ¤ngt von der konkreten Cluster-Konfiguration ab.


## ğŸ§¯ Fault recovery / Fehlerbehebung


Beim Einrichten deines Clusters solltest du bedenken, **wie dein Setup den Betrieb wiederherstellen kann**, falls **alle Failure Zones einer Region** gleichzeitig ausfallen.  
Beispielsweise: Vertraust du darauf, dass **mindestens ein Node** in einer Zone funktionsfÃ¤hig bleibt?

Stelle sicher, dass **kritische Reparaturprozesse** im Cluster **nicht davon abhÃ¤ngen**, dass mindestens ein gesunder Node vorhanden ist.  
Wenn alle Nodes fehlerhaft sind, musst du mÃ¶glicherweise einen **Repair Job mit spezieller Toleration** starten, um das Cluster teilweise wiederherzustellen, bis mindestens ein Node wieder einsatzbereit ist.

Kubernetes liefert hierfÃ¼r **keine integrierte LÃ¶sung**, doch du solltest diesen Aspekt bei der Planung berÃ¼cksichtigen.


## ğŸ”œ What's next / NÃ¤chste Schritte


Erfahre mehr darÃ¼ber, **wie der Scheduler Pods platziert** und dabei die konfigurierten **Constraints** berÃ¼cksichtigt:  
â¡ï¸ [Scheduling and Eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/)

