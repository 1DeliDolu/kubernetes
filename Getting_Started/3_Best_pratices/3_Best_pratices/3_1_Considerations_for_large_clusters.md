# Considerations for large clusters

## üöÄ Considerations for large clusters / √úberlegungen f√ºr gro√üe Cluster


**Deutsche √úbersetzung:**  
Ein Cluster ist eine Gruppe von Nodes (physische oder virtuelle Maschinen), auf denen Kubernetes-Agenten laufen und die durch die Control Plane verwaltet werden. Kubernetes v1.34 unterst√ºtzt Cluster mit bis zu **5.000 Nodes**. Genauer gesagt ist Kubernetes darauf ausgelegt, Konfigurationen zu unterst√ºtzen, die alle folgenden Kriterien erf√ºllen:

- Nicht mehr als **110 Pods pro Node**  
- Nicht mehr als **5.000 Nodes**  
- Nicht mehr als **150.000 Pods insgesamt**  
- Nicht mehr als **300.000 Container insgesamt**

Du kannst dein Cluster skalieren, indem du Nodes hinzuf√ºgst oder entfernst. Wie das geschieht, h√§ngt davon ab, wie dein Cluster bereitgestellt wurde.


## ‚òÅÔ∏è Cloud provider resource quotas / Ressourcenquoten des Cloud-Anbieters

Um zu vermeiden, dass du auf Quotenbeschr√§nkungen deines Cloud-Anbieters st√∂√üt, solltest du beim Erstellen eines Clusters mit vielen Nodes Folgendes ber√ºcksichtigen:

- Fordere eine **Quota-Erh√∂hung** f√ºr Cloud-Ressourcen an wie:
  - Compute-Instanzen  
  - CPUs  
  - Speicher-Volumes  
  - Verwendete IP-Adressen  
  - Paketfilter-Regels√§tze  
  - Anzahl der Load Balancer  
  - Netzwerk-Subnetze  
  - Log-Streams

- **Drossle die Skalierung des Clusters**, indem du neue Nodes in **Chargen** hinzuf√ºgst und zwischen den Chargen **Pausen** einlegst, da einige Cloud-Anbieter die Erstellung neuer Instanzen **rate-limitieren**.


## üß† Control plane components / Control-Plane-Komponenten

F√ºr ein gro√ües Cluster ben√∂tigst du eine Control Plane mit ausreichenden Rechen- und anderen Ressourcen.

Typischerweise betreibt man **eine oder zwei Control-Plane-Instanzen pro Failure Zone**, wobei man zun√§chst **vertikal skaliert** (mehr Ressourcen pro Instanz) und anschlie√üend **horizontal** (mehr Instanzen), sobald die vertikale Skalierung keine nennenswerten Leistungsgewinne mehr bringt.

Mindestens **eine Instanz pro Failure Zone** sollte bereitgestellt werden, um **Fehlertoleranz** zu gew√§hrleisten. Kubernetes-Nodes leiten den Verkehr **nicht automatisch** zu Control-Plane-Endpunkten in derselben Failure Zone um; allerdings kann dein Cloud-Anbieter eigene Mechanismen daf√ºr bereitstellen.

Ein Beispiel: Mit einem **verwalteten Load Balancer** kannst du den Verkehr aus der Failure Zone A so konfigurieren, dass er nur an Control-Plane-Hosts in Zone A geleitet wird. F√§llt eine Control-Plane-Instanz oder eine ganze Zone aus, wird der gesamte Control-Plane-Verkehr der betroffenen Zone **zwischen Zonen** geleitet. Durch mehrere Control-Plane-Hosts pro Zone l√§sst sich dieses Risiko reduzieren.


## üóÉÔ∏è etcd storage / etcd-Speicher

Zur Leistungsoptimierung gro√üer Cluster kannst du **Event-Objekte** in einer **separaten dedizierten etcd-Instanz** speichern.

Beim Erstellen eines Clusters kannst du (mithilfe eigener Tools):

- eine zus√§tzliche etcd-Instanz starten und konfigurieren  
- den **API-Server** so konfigurieren, dass er diese Instanz zur Speicherung von Events verwendet  

Siehe hierzu:
- [Operating etcd clusters for Kubernetes](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/)  
- [Set up a High Availability etcd cluster with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)


## üîß Addon resources / Addon-Ressourcen

Die **Ressourcenlimits** in Kubernetes helfen, die Auswirkungen von **Memory Leaks** und anderen Problemen zu begrenzen, durch die Pods und Container andere Komponenten beeintr√§chtigen k√∂nnen. Diese Limits gelten sowohl f√ºr **Addons** als auch f√ºr **Anwendungs-Workloads**.

Beispiel: Setzen von CPU- und Speicher-Limits f√ºr eine Logging-Komponente:

```yaml
...
containers:
- name: fluentd-cloud-logging
  image: fluent/fluentd-kubernetes-daemonset:v1
  resources:
    limits:
      cpu: 100m
      memory: 200Mi
````

Die Standard-Limits von Addons basieren in der Regel auf Erfahrungen mit **kleinen oder mittleren Clustern**. In gro√üen Clustern verbrauchen Addons h√§ufig **mehr Ressourcen** als ihre Standardgrenzen erlauben. Wenn du diese Werte nicht anpasst, kann es passieren, dass Addons:

* **st√§ndig beendet und neu gestartet** werden, weil sie das Memory-Limit erreichen, oder
* **schlechte Performance** aufweisen, weil CPU-Zeitfenster zu knapp bemessen sind.

Um solche Probleme zu vermeiden, beachte beim Erstellen gro√üer Cluster Folgendes:

* Einige Addons **skalieren vertikal** (eine Replik pro Cluster oder pro Failure Zone). Erh√∂he hier Requests und Limits, wenn das Cluster w√§chst.
* Viele Addons **skalieren horizontal** (mehr Pods = mehr Kapazit√§t). In sehr gro√üen Clustern solltest du zus√§tzlich CPU- und Memory-Limits leicht erh√∂hen.

  * Der **Vertical Pod Autoscaler** kann im **Recommender-Modus** Empfehlungen f√ºr Requests und Limits geben.
* Manche Addons laufen **pro Node** (z. B. Node-level Log-Aggregatoren √ºber eine `DaemonSet`). Auch hier kann eine leichte Erh√∂hung der Ressourcenlimits erforderlich sein.

## üîú What's next / N√§chste Schritte

Der **VerticalPodAutoscaler** ist eine benutzerdefinierte Ressource, die du in deinem Cluster bereitstellen kannst, um **Ressourcenanforderungen und -limits von Pods** automatisch zu verwalten.

Weitere Informationen:

* [Vertical Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) ‚Äì wie du damit Clusterkomponenten einschlie√ülich kritischer Addons skalieren kannst
* [Node autoscaling](https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#node-autoscaling)
* [Addon resizer](https://github.com/kubernetes/autoscaler/tree/master/addon-resizer) ‚Äì zum automatischen Anpassen von Addon-Gr√∂√üen bei Cluster-Skalierung

# []
